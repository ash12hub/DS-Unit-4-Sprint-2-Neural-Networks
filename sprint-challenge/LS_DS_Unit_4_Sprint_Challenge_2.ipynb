{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "Neurons are the nerve cells of the human body. These are the cells that are responsible for tranfering information throughout the body. When a signal is recived such as  a color entering they eye or a sound entering the ear, a neuron is fires a signal and the infromation is transmitted through other neurons.\n",
    "- **Input Layer:**\n",
    "This is the first part in a Neural Network. This is the layer that recieves information to be processed. This is the only part of a Neural Network that interacts with a user.\n",
    "- **Hidden Layer:**\n",
    "Once information has been recieved from the Input Layer, the Hidden Layer processes in the information in-order to calculate the output of the Neural Network. This output is then passed to the Output Layer. Processing information can involve passing through an activation function and calculating weights.\n",
    "- **Output Layer:**\n",
    "Once the Hidden Layer has processed the data from the Input Layer the calculated output in then passed the Output Layer where it is viewed by the user.\n",
    "- **Activation:**\n",
    "Not all the input from the Input Layer is required. Activation is a function decides how much information is actually passed into the Neural Network. It takes away all the input that isn't needed and only keep the input that is actually being used in the Neural Network.\n",
    "- **Backpropagation:**\n",
    "In a Neural Network Backpropagation is the process of updating weights at the end of each training loop of a model. The weights of each layer is updated in reverse order. When a Neural Network loop is finished, the error of the ouput is calculated and used to update the wieghts. Once the weights have been updated the model is run again using the new weights in order to produce a more accurate output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Start your candy perceptron here\n",
    "import numpy as np\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy[['ate']].values\n",
    "print(y.dtype)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "\n",
    "for i in range(1000):\n",
    "    weighted_sum = np.dot(X, weights)\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "    error = y - activated_output\n",
    "    if i == 999:\n",
    "        break\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "    weights = np.dot(X.T, adjustments)\n",
    "\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y, activated_output.astype(int))\n",
    "print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple or Single layer Perceptrons only have an input layer and an output layer. There are no hidden layers to provide further computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, input_nodes = 2, hidden_nodes = 3, output_nodes = 1):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __feed_forward(self, X):\n",
    "        self.weights1 = np.random.rand(self.input_nodes, self.hidden_nodes)\n",
    "        self.weights2 = np.random.rand(self.hidden_nodes, self.output_nodes)\n",
    "        \n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        self.activated_hidden = self.__sigmoid(self.hidden_sum)\n",
    "        \n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.__sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def __backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        self.o_delta = self.o_error * self.__sigmoid_derivative(o)\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        self.z2_delta = self.z2_error - self.__sigmoid_derivative(self.activated_hidden)\n",
    "        \n",
    "        self.weights1 = X.T.dot(self.z2_delta)\n",
    "        self.weights2 = self.activated_hidden.T.dot(self.o_delta)\n",
    "    \n",
    "    def train(self, X, y, n_iter = 10):\n",
    "        for _ in range(n_iter):\n",
    "            self.o = self.__feed_forward(X)\n",
    "            self.__backward(X, y, self.o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(input_nodes=X.shape[1], hidden_nodes=8)\n",
    "nn.train(X, y, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[0.93155647]\n",
      " [0.93540416]\n",
      " [0.93155647]\n",
      " ...\n",
      " [0.93155647]\n",
      " [0.93155647]\n",
      " [0.93540416]]\n"
     ]
    }
   ],
   "source": [
    "print('Output:', nn.o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [[0.06844353]\n",
      " [0.06459584]\n",
      " [0.06844353]\n",
      " ...\n",
      " [0.06844353]\n",
      " [0.06844353]\n",
      " [0.06459584]]\n"
     ]
    }
   ],
   "source": [
    "print('Error:', nn.o_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Multi-layer Perceptron hidden layers are used to handle further processing of the data from the input. By hidden layers the data is iteerated more times in order to calculate a more accurate output. Before it was running through a single iteration with a single set of weights to return an output that wasn't very accurate. Adding hidden layers allows the data to go through multiple iterations with more sets of weights, which end up producing a more accurate output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Not really"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "263   63    0   0       108   269    0        1      169      1      1.8   \n",
       "251   43    1   0       132   247    1        0      143      1      0.1   \n",
       "160   56    1   1       120   240    0        1      169      0      0.0   \n",
       "37    54    1   2       150   232    0        0      165      0      1.6   \n",
       "292   58    0   0       170   225    1        0      146      1      2.8   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "263      1   2     2       0  \n",
       "251      1   4     3       0  \n",
       "160      0   0     2       1  \n",
       "37       2   0     3       1  \n",
       "292      1   2     1       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashwin/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 227 samples\n",
      "Epoch 1/75\n",
      "227/227 [==============================] - 0s 2ms/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 2/75\n",
      "227/227 [==============================] - 0s 195us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 3/75\n",
      "227/227 [==============================] - 0s 761us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 4/75\n",
      "227/227 [==============================] - 0s 254us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 5/75\n",
      "227/227 [==============================] - 0s 191us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 6/75\n",
      "227/227 [==============================] - 0s 507us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 7/75\n",
      "227/227 [==============================] - 0s 496us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 8/75\n",
      "227/227 [==============================] - 0s 420us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 9/75\n",
      "227/227 [==============================] - 0s 286us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 10/75\n",
      "227/227 [==============================] - 0s 554us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 11/75\n",
      "227/227 [==============================] - 0s 229us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 12/75\n",
      "227/227 [==============================] - 0s 166us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 13/75\n",
      "227/227 [==============================] - 0s 170us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 14/75\n",
      "227/227 [==============================] - 0s 150us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 15/75\n",
      "227/227 [==============================] - 0s 166us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 16/75\n",
      "227/227 [==============================] - 0s 155us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 17/75\n",
      "227/227 [==============================] - 0s 327us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 18/75\n",
      "227/227 [==============================] - 0s 173us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 19/75\n",
      "227/227 [==============================] - 0s 339us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 20/75\n",
      "227/227 [==============================] - 0s 211us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 21/75\n",
      "227/227 [==============================] - 0s 271us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 22/75\n",
      "227/227 [==============================] - 0s 210us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 23/75\n",
      "227/227 [==============================] - 0s 211us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 24/75\n",
      "227/227 [==============================] - 0s 174us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 25/75\n",
      "227/227 [==============================] - 0s 197us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 26/75\n",
      "227/227 [==============================] - 0s 753us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 27/75\n",
      "227/227 [==============================] - 0s 616us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 28/75\n",
      "227/227 [==============================] - 0s 272us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 29/75\n",
      "227/227 [==============================] - 0s 192us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 30/75\n",
      "227/227 [==============================] - 0s 163us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 31/75\n",
      "227/227 [==============================] - 0s 353us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 32/75\n",
      "227/227 [==============================] - 0s 534us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 33/75\n",
      "227/227 [==============================] - 0s 297us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 34/75\n",
      "227/227 [==============================] - 0s 130us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 35/75\n",
      "227/227 [==============================] - 0s 132us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 36/75\n",
      "227/227 [==============================] - 0s 138us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 37/75\n",
      "227/227 [==============================] - 0s 232us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 38/75\n",
      "227/227 [==============================] - 0s 154us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 39/75\n",
      "227/227 [==============================] - 0s 146us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 40/75\n",
      "227/227 [==============================] - 0s 118us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 41/75\n",
      "227/227 [==============================] - 0s 143us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 42/75\n",
      "227/227 [==============================] - 0s 128us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 43/75\n",
      "227/227 [==============================] - 0s 204us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 44/75\n",
      "227/227 [==============================] - 0s 140us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 45/75\n",
      "227/227 [==============================] - 0s 136us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 46/75\n",
      "227/227 [==============================] - 0s 272us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 47/75\n",
      "227/227 [==============================] - 0s 159us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 48/75\n",
      "227/227 [==============================] - 0s 146us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 49/75\n",
      "227/227 [==============================] - 0s 228us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 50/75\n",
      "227/227 [==============================] - 0s 156us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 51/75\n",
      "227/227 [==============================] - 0s 154us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 52/75\n",
      "227/227 [==============================] - 0s 152us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 53/75\n",
      "227/227 [==============================] - 0s 175us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 54/75\n",
      "227/227 [==============================] - 0s 176us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 55/75\n",
      "227/227 [==============================] - 0s 212us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 56/75\n",
      "227/227 [==============================] - 0s 473us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 57/75\n",
      "227/227 [==============================] - 0s 238us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 58/75\n",
      "227/227 [==============================] - 0s 595us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 59/75\n",
      "227/227 [==============================] - 0s 677us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 60/75\n",
      "227/227 [==============================] - 0s 266us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 61/75\n",
      "227/227 [==============================] - 0s 183us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 62/75\n",
      "227/227 [==============================] - 0s 172us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 63/75\n",
      "227/227 [==============================] - 0s 190us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 64/75\n",
      "227/227 [==============================] - 0s 473us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 65/75\n",
      "227/227 [==============================] - 0s 293us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 66/75\n",
      "227/227 [==============================] - 0s 132us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 67/75\n",
      "227/227 [==============================] - 0s 177us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 68/75\n",
      "227/227 [==============================] - 0s 175us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 69/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 0s 172us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 70/75\n",
      "227/227 [==============================] - 0s 165us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 71/75\n",
      "227/227 [==============================] - 0s 128us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 72/75\n",
      "227/227 [==============================] - 0s 118us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 73/75\n",
      "227/227 [==============================] - 0s 155us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 74/75\n",
      "227/227 [==============================] - 0s 143us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n",
      "Epoch 75/75\n",
      "227/227 [==============================] - 0s 147us/sample - loss: 0.4670 - accuracy: 0.5330 - mae: 0.4670\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "X = df[['age', 'cp', 'trestbps', 'chol']]\n",
    "y = df[['target']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scalar.fit(x_train, y_train)\n",
    "\n",
    "model.add(Dense(4, input_dim=4, activation='sigmoid'))\n",
    "model.add(Dense(5, input_dim=4, activation='relu'))\n",
    "model.add(Dense(5, input_dim=4, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['accuracy', 'mae'])\n",
    "history = model.fit(x_train, y_train, batch_size=10, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4669603618756265,\n",
       "  0.4669603553112383,\n",
       "  0.46696036489524506,\n",
       "  0.46696035898729565,\n",
       "  0.4669603593155151,\n",
       "  0.46696036056274887,\n",
       "  0.4669603593155151,\n",
       "  0.4669603651578206,\n",
       "  0.46696035728055474,\n",
       "  0.46696035662411595,\n",
       "  0.46696036121918766,\n",
       "  0.4669603602345294,\n",
       "  0.4669603602345294,\n",
       "  0.4669603595780906,\n",
       "  0.46696035767441807,\n",
       "  0.46696036161305093,\n",
       "  0.4669603592498712,\n",
       "  0.46696036121918766,\n",
       "  0.4669603603001733,\n",
       "  0.4669603592498712,\n",
       "  0.46696036128483154,\n",
       "  0.46696036062839275,\n",
       "  0.4669603566897598,\n",
       "  0.46696035760877413,\n",
       "  0.46696035833085686,\n",
       "  0.4669603625977092,\n",
       "  0.4669603618756265,\n",
       "  0.4669603570179792,\n",
       "  0.46696036161305093,\n",
       "  0.4669603595780906,\n",
       "  0.4669603602345294,\n",
       "  0.46696036121918766,\n",
       "  0.46696036318850415,\n",
       "  0.46696035865907626,\n",
       "  0.46696035728055474,\n",
       "  0.4669603553112383,\n",
       "  0.4669603592498712,\n",
       "  0.46696035984066614,\n",
       "  0.4669603622038459,\n",
       "  0.4669603570179792,\n",
       "  0.46696036095661214,\n",
       "  0.4669603618756265,\n",
       "  0.46696035990631,\n",
       "  0.4669603622694898,\n",
       "  0.46696036154740705,\n",
       "  0.46696035695233534,\n",
       "  0.46696035898729565,\n",
       "  0.46696035990631,\n",
       "  0.4669603603001733,\n",
       "  0.4669603599719539,\n",
       "  0.46696036056274887,\n",
       "  0.46696035990631,\n",
       "  0.4669603603001733,\n",
       "  0.46696035800263747,\n",
       "  0.46696035800263747,\n",
       "  0.4669603592498712,\n",
       "  0.46696035833085686,\n",
       "  0.46696035636154043,\n",
       "  0.4669603603001733,\n",
       "  0.4669603603001733,\n",
       "  0.46696036089096826,\n",
       "  0.4669603599719539,\n",
       "  0.46696035439222394,\n",
       "  0.46696036548604,\n",
       "  0.46696035898729565,\n",
       "  0.46696035865907626,\n",
       "  0.4669603592498712,\n",
       "  0.4669603592498712,\n",
       "  0.4669603602345294,\n",
       "  0.46696035662411595,\n",
       "  0.4669603603001733,\n",
       "  0.4669603625320653,\n",
       "  0.4669603570179792,\n",
       "  0.46696036056274887,\n",
       "  0.46696035767441807],\n",
       " 'accuracy': [0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396,\n",
       "  0.5330396],\n",
       " 'mae': [0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034,\n",
       "  0.46696034]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'batch_size': [10, 20],\n",
    "              'epoch': [70]}\n",
    "grid_model = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashwin/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3d255278>' (type <class 'tensorflow.python.keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fa850eb7e79c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     58\u001b[0m                             \u001b[0;34m\"it does not seem to be a scikit-learn estimator \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                             \u001b[0;34m\"as it does not implement a 'get_params' methods.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                             % (repr(estimator), type(estimator)))\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3d255278>' (type <class 'tensorflow.python.keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods."
     ]
    }
   ],
   "source": [
    "grid_model.fit(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
